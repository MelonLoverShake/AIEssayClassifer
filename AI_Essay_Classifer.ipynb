{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t0Qpbv29gud-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import xgboost as xgb\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_robust(filepath):\n",
        "    \"\"\"\n",
        "    Robust CSV loading with multiple fallback strategies\n",
        "    Args:\n",
        "        filepath: Path to CSV file\n",
        "    Returns:\n",
        "        DataFrame\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load: {filepath}\")\n",
        "\n",
        "    # Strategy 1: Standard pandas read_csv\n",
        "    try:\n",
        "        print(\"Trying standard CSV loading...\")\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"✓ Successfully loaded with standard method\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Standard loading failed: {str(e)[:100]}\")\n",
        "\n",
        "    # Strategy 2: Handle quote issues with error_bad_lines parameter\n",
        "    try:\n",
        "        print(\"Trying with on_bad_lines='skip'...\")\n",
        "        df = pd.read_csv(filepath, on_bad_lines='skip', engine='python')\n",
        "        print(f\"✓ Successfully loaded, some bad lines were skipped\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed: {str(e)[:100]}\")\n",
        "\n",
        "    # Strategy 3: Handle malformed quotes\n",
        "    try:\n",
        "        print(\"Trying with quoting=csv.QUOTE_NONE...\")\n",
        "        import csv\n",
        "        df = pd.read_csv(filepath, quoting=csv.QUOTE_NONE, engine='python', on_bad_lines='skip')\n",
        "        print(f\"✓ Successfully loaded with no quoting\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed: {str(e)[:100]}\")\n",
        "\n",
        "    # Strategy 4: Custom delimiter and encoding\n",
        "    try:\n",
        "        print(\"Trying with different encoding (latin-1)...\")\n",
        "        df = pd.read_csv(filepath, encoding='latin-1', on_bad_lines='skip', engine='python')\n",
        "        print(f\"✓ Successfully loaded with latin-1 encoding\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed: {str(e)[:100]}\")\n",
        "\n",
        "    # Strategy 5: Read line by line (slowest but most robust)\n",
        "    try:\n",
        "        print(\"Trying line-by-line parsing (this may take a while)...\")\n",
        "        data = []\n",
        "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            header = f.readline().strip().split(',')\n",
        "            print(f\"Header: {header}\")\n",
        "\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    # Simple splitting - assumes 2 columns\n",
        "                    parts = line.strip().rsplit(',', 1)  # Split from right to get last column\n",
        "                    if len(parts) == 2:\n",
        "                        text = parts[0].strip('\"').strip(\"'\")\n",
        "                        label = parts[1].strip()\n",
        "                        data.append({'text': text, 'generated': label})\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "                if (i + 1) % 10000 == 0:\n",
        "                    print(f\"Processed {i + 1} lines...\")\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"✓ Successfully loaded {len(df)} rows with line-by-line parsing\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Line-by-line parsing failed: {str(e)[:100]}\")\n",
        "\n",
        "    raise ValueError(\"All loading strategies failed. Please check your CSV file format.\")\n",
        "\n",
        "def load_and_preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Load and preprocess the dataset\n",
        "    Args:\n",
        "        df: DataFrame with 'text' and 'generated' columns\n",
        "    Returns:\n",
        "        Preprocessed DataFrame\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA PREPROCESSING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nInitial shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Handle different column name cases\n",
        "    df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "    # Check for required columns\n",
        "    if 'text' not in df.columns or 'generated' not in df.columns:\n",
        "        print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
        "        raise ValueError(\"DataFrame must have 'text' and 'generated' columns\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=['text'])\n",
        "    print(f\"Removed {before - len(df)} duplicate rows\")\n",
        "\n",
        "    # Remove null values\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=['text', 'generated'])\n",
        "    print(f\"Removed {before - len(df)} rows with null values\")\n",
        "\n",
        "    # Convert to proper types\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Clean generated column (handle various formats)\n",
        "    df['generated'] = df['generated'].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Map to binary (handle different formats: 0/1, true/false, yes/no, etc.)\n",
        "    def map_to_binary(val):\n",
        "        val = str(val).lower().strip()\n",
        "        if val in ['1', '1.0', 'true', 'yes', 'ai', 'generated']:\n",
        "            return 1\n",
        "        elif val in ['0', '0.0', 'false', 'no', 'human']:\n",
        "            return 0\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    df['generated'] = df['generated'].apply(map_to_binary)\n",
        "\n",
        "    # Remove rows where mapping failed\n",
        "    before = len(df)\n",
        "    df = df.dropna(subset=['generated'])\n",
        "    df['generated'] = df['generated'].astype(int)\n",
        "    print(f\"Removed {before - len(df)} rows with invalid labels\")\n",
        "\n",
        "    # Remove very short texts (less than 50 characters)\n",
        "    before = len(df)\n",
        "    df = df[df['text'].str.len() > 50]\n",
        "    print(f\"Removed {before - len(df)} rows with text < 50 characters\")\n",
        "\n",
        "    # Remove very long texts that might cause issues (> 10000 chars)\n",
        "    before = len(df)\n",
        "    df = df[df['text'].str.len() <= 10000]\n",
        "    print(f\"Removed {before - len(df)} rows with text > 10000 characters\")\n",
        "\n",
        "    # Reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "    print(f\"\\nClass distribution:\")\n",
        "    print(df['generated'].value_counts())\n",
        "    print(f\"\\nClass distribution (%):\")\n",
        "    print(df['generated'].value_counts(normalize=True)*100)\n",
        "\n",
        "    # Check for class imbalance\n",
        "    class_counts = df['generated'].value_counts()\n",
        "    ratio = class_counts.max() / class_counts.min()\n",
        "    if ratio > 3:\n",
        "        print(f\"\\n⚠ WARNING: Significant class imbalance detected (ratio: {ratio:.2f})\")\n",
        "        print(\"Consider using class_weight='balanced' in models\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "WN6SHPDTg3PG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_linguistic_features(text):\n",
        "    \"\"\"Extract linguistic features that might distinguish AI from human text\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Length features\n",
        "    features['char_count'] = len(text)\n",
        "    features['word_count'] = len(text.split())\n",
        "    features['avg_word_length'] = np.mean([len(word) for word in text.split()])\n",
        "\n",
        "    # Sentence features\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s for s in sentences if s.strip()]\n",
        "    features['sentence_count'] = len(sentences)\n",
        "    features['avg_sentence_length'] = np.mean([len(s.split()) for s in sentences]) if sentences else 0\n",
        "\n",
        "    # Punctuation features\n",
        "    features['comma_count'] = text.count(',')\n",
        "    features['semicolon_count'] = text.count(';')\n",
        "    features['exclamation_count'] = text.count('!')\n",
        "    features['question_count'] = text.count('?')\n",
        "\n",
        "    # Vocabulary diversity\n",
        "    words = text.lower().split()\n",
        "    features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0\n",
        "\n",
        "    # AI text often has more consistent structure\n",
        "    features['sentence_length_variance'] = np.var([len(s.split()) for s in sentences]) if len(sentences) > 1 else 0\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_feature_dataframe(df):\n",
        "    \"\"\"Create feature DataFrame from text\"\"\"\n",
        "    features_list = df['text'].apply(extract_linguistic_features).tolist()\n",
        "    features_df = pd.DataFrame(features_list)\n",
        "    return features_df"
      ],
      "metadata": {
        "id": "sFYYaXoNhLMq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_traditional_ml_models(X_train, X_test, y_train, y_test, text_train, text_test):\n",
        "    \"\"\"\n",
        "    Train ensemble of traditional ML models with TF-IDF features\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING TRADITIONAL ML MODELS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    print(\"\\nCreating TF-IDF features...\")\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 3),\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    X_train_tfidf = tfidf.fit_transform(text_train)\n",
        "    X_test_tfidf = tfidf.transform(text_test)\n",
        "\n",
        "    # Combine TF-IDF with linguistic features\n",
        "    from scipy.sparse import hstack\n",
        "    X_train_combined = hstack([X_train_tfidf, X_train.values])\n",
        "    X_test_combined = hstack([X_test_tfidf, X_test.values])\n",
        "\n",
        "    # Model 1: Logistic Regression\n",
        "    print(\"\\nTraining Logistic Regression...\")\n",
        "    lr_model = LogisticRegression(C=2.0, max_iter=1000, class_weight='balanced', random_state=42)\n",
        "    lr_model.fit(X_train_combined, y_train)\n",
        "    lr_pred = lr_model.predict(X_test_combined)\n",
        "    lr_acc = accuracy_score(y_test, lr_pred)\n",
        "    print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "\n",
        "    # Model 2: XGBoost\n",
        "    print(\"\\nTraining XGBoost...\")\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=7,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "    xgb_model.fit(X_train_combined, y_train)\n",
        "    xgb_pred = xgb_model.predict(X_test_combined)\n",
        "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "    print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
        "\n",
        "    # Model 3: Random Forest\n",
        "    print(\"\\nTraining Random Forest...\")\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_split=5,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    rf_model.fit(X_train_combined, y_train)\n",
        "    rf_pred = rf_model.predict(X_test_combined)\n",
        "    rf_acc = accuracy_score(y_test, rf_pred)\n",
        "    print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "\n",
        "    # Ensemble with soft voting\n",
        "    print(\"\\nCreating Ensemble Model...\")\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('xgb', xgb_model),\n",
        "            ('rf', rf_model)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "    ensemble.fit(X_train_combined, y_train)\n",
        "    ensemble_pred = ensemble.predict(X_test_combined)\n",
        "    ensemble_acc = accuracy_score(y_test, ensemble_pred)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TRADITIONAL ML RESULTS\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, ensemble_pred, target_names=['Human', 'AI']))\n",
        "    print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, ensemble.predict_proba(X_test_combined)[:, 1]):.4f}\")\n",
        "\n",
        "    return ensemble, tfidf, ensemble_acc"
      ],
      "metadata": {
        "id": "1iIyT0NqhPVk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for transformer models\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def train_transformer_model(text_train, text_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train a transformer-based model (DistilBERT for efficiency)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING TRANSFORMER MODEL\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use DistilBERT for faster training while maintaining high accuracy\n",
        "    model_name = 'distilbert-base-uncased'\n",
        "\n",
        "    print(f\"\\nLoading tokenizer and model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(\n",
        "        text_train.tolist(),\n",
        "        y_train.tolist(),\n",
        "        tokenizer,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "        text_test.tolist(),\n",
        "        y_test.tolist(),\n",
        "        tokenizer,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        learning_rate=2e-5,\n",
        "    )\n",
        "\n",
        "    # Metrics\n",
        "    def compute_metrics(pred):\n",
        "        labels = pred.label_ids\n",
        "        preds = pred.predictions.argmax(-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    transformer_acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TRANSFORMER MODEL RESULTS\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Accuracy: {transformer_acc:.4f}\")\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, preds, target_names=['Human', 'AI']))\n",
        "    print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, torch.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()):.4f}\")\n",
        "\n",
        "    return model, tokenizer, transformer_acc"
      ],
      "metadata": {
        "id": "U028BtOohZf0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_full_pipeline(df):\n",
        "    \"\"\"\n",
        "    Run the complete pipeline\n",
        "    Args:\n",
        "        df: DataFrame with 'text' and 'generated' columns\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"AI TEXT DETECTION PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Preprocess data\n",
        "    df = load_and_preprocess_data(df)\n",
        "\n",
        "    # Step 2: Create linguistic features\n",
        "    print(\"\\nExtracting linguistic features...\")\n",
        "    features_df = create_feature_dataframe(df)\n",
        "\n",
        "    # Step 3: Split data (stratified to maintain class balance)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features_df,\n",
        "        df['generated'],\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['generated']\n",
        "    )\n",
        "\n",
        "    text_train = df.loc[X_train.index, 'text'].reset_index(drop=True)\n",
        "    text_test = df.loc[X_test.index, 'text'].reset_index(drop=True)\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    # Step 4: Train traditional ML models\n",
        "    ensemble_model, tfidf, ensemble_acc = train_traditional_ml_models(\n",
        "        X_train, X_test, y_train, y_test, text_train, text_test\n",
        "    )\n",
        "\n",
        "    # Step 5: Train transformer model\n",
        "    transformer_model, tokenizer, transformer_acc = train_transformer_model(\n",
        "        text_train, text_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Traditional ML Ensemble Accuracy: {ensemble_acc:.4f} ({ensemble_acc*100:.2f}%)\")\n",
        "    print(f\"Transformer Model Accuracy: {transformer_acc:.4f} ({transformer_acc*100:.2f}%)\")\n",
        "    print(f\"\\nBest Model: {'Transformer' if transformer_acc > ensemble_acc else 'Traditional ML Ensemble'}\")\n",
        "    print(f\"Best Accuracy: {max(ensemble_acc, transformer_acc):.4f} ({max(ensemble_acc, transformer_acc)*100:.2f}%)\")\n",
        "\n",
        "    if max(ensemble_acc, transformer_acc) >= 0.95:\n",
        "        print(\"\\n✓ TARGET ACHIEVED: >95% accuracy\")\n",
        "    else:\n",
        "        print(\"\\n✗ TARGET NOT MET: Consider:\")\n",
        "        print(\"  - More training data\")\n",
        "        print(\"  - Longer training (more epochs)\")\n",
        "        print(\"  - Larger transformer model (roberta-base, bert-large)\")\n",
        "        print(\"  - Hyperparameter tuning\")\n",
        "\n",
        "    return {\n",
        "        'ensemble_model': ensemble_model,\n",
        "        'tfidf': tfidf,\n",
        "        'transformer_model': transformer_model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'ensemble_acc': ensemble_acc,\n",
        "        'transformer_acc': transformer_acc\n",
        "    }"
      ],
      "metadata": {
        "id": "4YiDEOU1hfQp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will clean and save a new CSV\n",
        "def fix_csv_file(input_path, output_path):\n",
        "    print(f\"Fixing CSV: {input_path}\")\n",
        "    data = []\n",
        "    errors = 0\n",
        "\n",
        "    with open(input_path, 'r', encoding='utf-8', errors='ignore') as infile:\n",
        "        # Read header\n",
        "        header = infile.readline().strip()\n",
        "\n",
        "        for i, line in enumerate(infile):\n",
        "            try:\n",
        "                # Remove any problematic characters\n",
        "                line = line.strip()\n",
        "\n",
        "                # Try to parse - assuming last comma separates text from label\n",
        "                parts = line.rsplit(',', 1)\n",
        "                if len(parts) == 2:\n",
        "                    text = parts[0].strip('\"').strip(\"'\").replace('\"', '\"\"')\n",
        "                    label = parts[1].strip()\n",
        "                    data.append(f'\"{text}\",{label}')\n",
        "            except:\n",
        "                errors += 1\n",
        "                continue\n",
        "\n",
        "            if (i + 1) % 10000 == 0:\n",
        "                print(f\"Processed {i + 1} lines, {errors} errors\")\n",
        "\n",
        "    # Write cleaned CSV\n",
        "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write('text,generated\\n')\n",
        "        for row in data:\n",
        "            outfile.write(row + '\\n')\n",
        "\n",
        "    print(f\"\\nFixed CSV saved to: {output_path}\")\n",
        "    print(f\"Total rows: {len(data)}, Errors: {errors}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "fixed_file = fix_csv_file('AI_Human.csv', 'fixed_data.csv')\n",
        "df = pd.read_csv(fixed_file)\n",
        "results = run_full_pipeline(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNOHGRLihgyU",
        "outputId": "60b912d9-38b8-4a2a-eff4-652bd4efaed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixing CSV: AI_Human.csv\n",
            "Processed 10000 lines, 0 errors\n",
            "Processed 20000 lines, 0 errors\n",
            "Processed 30000 lines, 0 errors\n",
            "Processed 40000 lines, 0 errors\n",
            "Processed 50000 lines, 0 errors\n",
            "Processed 60000 lines, 0 errors\n",
            "Processed 70000 lines, 0 errors\n",
            "Processed 80000 lines, 0 errors\n",
            "Processed 90000 lines, 0 errors\n",
            "Processed 100000 lines, 0 errors\n",
            "Processed 110000 lines, 0 errors\n",
            "Processed 120000 lines, 0 errors\n",
            "Processed 130000 lines, 0 errors\n",
            "Processed 140000 lines, 0 errors\n",
            "Processed 150000 lines, 0 errors\n",
            "Processed 160000 lines, 0 errors\n",
            "Processed 170000 lines, 0 errors\n",
            "Processed 180000 lines, 0 errors\n",
            "Processed 190000 lines, 0 errors\n",
            "Processed 200000 lines, 0 errors\n",
            "Processed 210000 lines, 0 errors\n",
            "Processed 220000 lines, 0 errors\n",
            "Processed 230000 lines, 0 errors\n",
            "Processed 240000 lines, 0 errors\n",
            "Processed 250000 lines, 0 errors\n",
            "Processed 260000 lines, 0 errors\n",
            "Processed 270000 lines, 0 errors\n",
            "Processed 280000 lines, 0 errors\n",
            "Processed 290000 lines, 0 errors\n",
            "Processed 300000 lines, 0 errors\n",
            "Processed 310000 lines, 0 errors\n",
            "Processed 320000 lines, 0 errors\n",
            "Processed 330000 lines, 0 errors\n",
            "Processed 340000 lines, 0 errors\n",
            "Processed 350000 lines, 0 errors\n",
            "Processed 360000 lines, 0 errors\n",
            "Processed 370000 lines, 0 errors\n",
            "Processed 380000 lines, 0 errors\n",
            "Processed 390000 lines, 0 errors\n",
            "Processed 400000 lines, 0 errors\n",
            "Processed 410000 lines, 0 errors\n",
            "Processed 420000 lines, 0 errors\n",
            "Processed 430000 lines, 0 errors\n",
            "Processed 440000 lines, 0 errors\n",
            "Processed 450000 lines, 0 errors\n",
            "Processed 460000 lines, 0 errors\n",
            "Processed 470000 lines, 0 errors\n",
            "Processed 480000 lines, 0 errors\n",
            "Processed 490000 lines, 0 errors\n",
            "Processed 500000 lines, 0 errors\n",
            "Processed 510000 lines, 0 errors\n",
            "Processed 520000 lines, 0 errors\n",
            "Processed 530000 lines, 0 errors\n",
            "Processed 540000 lines, 0 errors\n",
            "Processed 550000 lines, 0 errors\n",
            "Processed 560000 lines, 0 errors\n",
            "Processed 570000 lines, 0 errors\n",
            "Processed 580000 lines, 0 errors\n",
            "Processed 590000 lines, 0 errors\n",
            "Processed 600000 lines, 0 errors\n",
            "Processed 610000 lines, 0 errors\n",
            "Processed 620000 lines, 0 errors\n",
            "Processed 630000 lines, 0 errors\n",
            "Processed 640000 lines, 0 errors\n",
            "Processed 650000 lines, 0 errors\n",
            "Processed 660000 lines, 0 errors\n",
            "Processed 670000 lines, 0 errors\n",
            "Processed 680000 lines, 0 errors\n",
            "Processed 690000 lines, 0 errors\n",
            "Processed 700000 lines, 0 errors\n",
            "Processed 710000 lines, 0 errors\n",
            "Processed 720000 lines, 0 errors\n",
            "Processed 730000 lines, 0 errors\n",
            "Processed 740000 lines, 0 errors\n",
            "Processed 750000 lines, 0 errors\n",
            "Processed 760000 lines, 0 errors\n",
            "Processed 770000 lines, 0 errors\n",
            "Processed 780000 lines, 0 errors\n",
            "Processed 790000 lines, 0 errors\n",
            "Processed 800000 lines, 0 errors\n",
            "Processed 810000 lines, 0 errors\n",
            "Processed 820000 lines, 0 errors\n",
            "Processed 830000 lines, 0 errors\n",
            "Processed 840000 lines, 0 errors\n",
            "Processed 850000 lines, 0 errors\n",
            "Processed 860000 lines, 0 errors\n",
            "Processed 870000 lines, 0 errors\n",
            "Processed 880000 lines, 0 errors\n",
            "Processed 890000 lines, 0 errors\n",
            "Processed 900000 lines, 0 errors\n",
            "Processed 910000 lines, 0 errors\n",
            "Processed 920000 lines, 0 errors\n",
            "Processed 930000 lines, 0 errors\n",
            "Processed 940000 lines, 0 errors\n",
            "Processed 950000 lines, 0 errors\n",
            "Processed 960000 lines, 0 errors\n",
            "Processed 970000 lines, 0 errors\n",
            "Processed 980000 lines, 0 errors\n",
            "Processed 990000 lines, 0 errors\n",
            "Processed 1000000 lines, 0 errors\n",
            "Processed 1010000 lines, 0 errors\n",
            "Processed 1020000 lines, 0 errors\n",
            "Processed 1030000 lines, 0 errors\n",
            "Processed 1040000 lines, 0 errors\n",
            "Processed 1050000 lines, 0 errors\n",
            "Processed 1060000 lines, 0 errors\n",
            "Processed 1070000 lines, 0 errors\n",
            "Processed 1080000 lines, 0 errors\n",
            "Processed 1090000 lines, 0 errors\n",
            "Processed 1100000 lines, 0 errors\n",
            "Processed 1110000 lines, 0 errors\n",
            "Processed 1120000 lines, 0 errors\n",
            "Processed 1130000 lines, 0 errors\n",
            "Processed 1140000 lines, 0 errors\n",
            "Processed 1150000 lines, 0 errors\n",
            "Processed 1160000 lines, 0 errors\n",
            "Processed 1170000 lines, 0 errors\n",
            "Processed 1180000 lines, 0 errors\n",
            "Processed 1190000 lines, 0 errors\n",
            "Processed 1200000 lines, 0 errors\n",
            "Processed 1210000 lines, 0 errors\n",
            "Processed 1220000 lines, 0 errors\n",
            "Processed 1230000 lines, 0 errors\n",
            "Processed 1240000 lines, 0 errors\n",
            "Processed 1250000 lines, 0 errors\n",
            "Processed 1260000 lines, 0 errors\n",
            "Processed 1270000 lines, 0 errors\n",
            "Processed 1280000 lines, 0 errors\n",
            "Processed 1290000 lines, 0 errors\n",
            "Processed 1300000 lines, 0 errors\n",
            "Processed 1310000 lines, 0 errors\n",
            "Processed 1320000 lines, 0 errors\n",
            "Processed 1330000 lines, 0 errors\n",
            "Processed 1340000 lines, 0 errors\n",
            "Processed 1350000 lines, 0 errors\n",
            "Processed 1360000 lines, 0 errors\n",
            "Processed 1370000 lines, 0 errors\n",
            "Processed 1380000 lines, 0 errors\n",
            "Processed 1390000 lines, 0 errors\n",
            "Processed 1400000 lines, 0 errors\n",
            "Processed 1410000 lines, 0 errors\n",
            "Processed 1420000 lines, 0 errors\n",
            "Processed 1430000 lines, 0 errors\n",
            "Processed 1440000 lines, 0 errors\n",
            "Processed 1450000 lines, 0 errors\n",
            "Processed 1460000 lines, 0 errors\n",
            "Processed 1470000 lines, 0 errors\n",
            "Processed 1480000 lines, 0 errors\n",
            "Processed 1490000 lines, 0 errors\n",
            "Processed 1500000 lines, 0 errors\n",
            "Processed 1510000 lines, 0 errors\n",
            "Processed 1520000 lines, 0 errors\n",
            "Processed 1530000 lines, 0 errors\n",
            "Processed 1540000 lines, 0 errors\n",
            "Processed 1550000 lines, 0 errors\n",
            "Processed 1560000 lines, 0 errors\n",
            "Processed 1570000 lines, 0 errors\n",
            "Processed 1580000 lines, 0 errors\n",
            "Processed 1590000 lines, 0 errors\n",
            "Processed 1600000 lines, 0 errors\n",
            "Processed 1610000 lines, 0 errors\n",
            "Processed 1620000 lines, 0 errors\n",
            "Processed 1630000 lines, 0 errors\n",
            "Processed 1640000 lines, 0 errors\n",
            "Processed 1650000 lines, 0 errors\n",
            "Processed 1660000 lines, 0 errors\n",
            "Processed 1670000 lines, 0 errors\n",
            "Processed 1680000 lines, 0 errors\n",
            "Processed 1690000 lines, 0 errors\n",
            "Processed 1700000 lines, 0 errors\n",
            "Processed 1710000 lines, 0 errors\n",
            "Processed 1720000 lines, 0 errors\n",
            "Processed 1730000 lines, 0 errors\n",
            "Processed 1740000 lines, 0 errors\n",
            "Processed 1750000 lines, 0 errors\n",
            "Processed 1760000 lines, 0 errors\n",
            "Processed 1770000 lines, 0 errors\n",
            "Processed 1780000 lines, 0 errors\n",
            "Processed 1790000 lines, 0 errors\n",
            "Processed 1800000 lines, 0 errors\n",
            "Processed 1810000 lines, 0 errors\n",
            "Processed 1820000 lines, 0 errors\n",
            "Processed 1830000 lines, 0 errors\n",
            "Processed 1840000 lines, 0 errors\n",
            "Processed 1850000 lines, 0 errors\n",
            "Processed 1860000 lines, 0 errors\n",
            "Processed 1870000 lines, 0 errors\n",
            "Processed 1880000 lines, 0 errors\n",
            "Processed 1890000 lines, 0 errors\n",
            "Processed 1900000 lines, 0 errors\n",
            "Processed 1910000 lines, 0 errors\n",
            "Processed 1920000 lines, 0 errors\n",
            "Processed 1930000 lines, 0 errors\n",
            "Processed 1940000 lines, 0 errors\n",
            "Processed 1950000 lines, 0 errors\n",
            "Processed 1960000 lines, 0 errors\n",
            "Processed 1970000 lines, 0 errors\n",
            "Processed 1980000 lines, 0 errors\n",
            "Processed 1990000 lines, 0 errors\n",
            "Processed 2000000 lines, 0 errors\n",
            "Processed 2010000 lines, 0 errors\n",
            "Processed 2020000 lines, 0 errors\n",
            "Processed 2030000 lines, 0 errors\n",
            "Processed 2040000 lines, 0 errors\n",
            "Processed 2050000 lines, 0 errors\n",
            "Processed 2060000 lines, 0 errors\n",
            "Processed 2070000 lines, 0 errors\n",
            "Processed 2080000 lines, 0 errors\n",
            "Processed 2090000 lines, 0 errors\n",
            "Processed 2100000 lines, 0 errors\n",
            "Processed 2110000 lines, 0 errors\n",
            "Processed 2120000 lines, 0 errors\n",
            "Processed 2130000 lines, 0 errors\n",
            "Processed 2140000 lines, 0 errors\n",
            "Processed 2150000 lines, 0 errors\n",
            "Processed 2160000 lines, 0 errors\n",
            "Processed 2170000 lines, 0 errors\n",
            "Processed 2180000 lines, 0 errors\n",
            "Processed 2190000 lines, 0 errors\n",
            "Processed 2200000 lines, 0 errors\n",
            "Processed 2210000 lines, 0 errors\n",
            "Processed 2220000 lines, 0 errors\n",
            "Processed 2230000 lines, 0 errors\n",
            "Processed 2240000 lines, 0 errors\n",
            "Processed 2250000 lines, 0 errors\n",
            "Processed 2260000 lines, 0 errors\n",
            "Processed 2270000 lines, 0 errors\n",
            "Processed 2280000 lines, 0 errors\n",
            "Processed 2290000 lines, 0 errors\n",
            "Processed 2300000 lines, 0 errors\n",
            "Processed 2310000 lines, 0 errors\n",
            "Processed 2320000 lines, 0 errors\n",
            "Processed 2330000 lines, 0 errors\n",
            "Processed 2340000 lines, 0 errors\n",
            "Processed 2350000 lines, 0 errors\n",
            "Processed 2360000 lines, 0 errors\n",
            "Processed 2370000 lines, 0 errors\n",
            "Processed 2380000 lines, 0 errors\n",
            "Processed 2390000 lines, 0 errors\n",
            "Processed 2400000 lines, 0 errors\n",
            "Processed 2410000 lines, 0 errors\n",
            "Processed 2420000 lines, 0 errors\n",
            "Processed 2430000 lines, 0 errors\n",
            "Processed 2440000 lines, 0 errors\n",
            "Processed 2450000 lines, 0 errors\n",
            "Processed 2460000 lines, 0 errors\n",
            "Processed 2470000 lines, 0 errors\n",
            "Processed 2480000 lines, 0 errors\n",
            "Processed 2490000 lines, 0 errors\n",
            "Processed 2500000 lines, 0 errors\n",
            "Processed 2510000 lines, 0 errors\n",
            "Processed 2520000 lines, 0 errors\n",
            "Processed 2530000 lines, 0 errors\n",
            "Processed 2540000 lines, 0 errors\n",
            "Processed 2550000 lines, 0 errors\n",
            "Processed 2560000 lines, 0 errors\n",
            "Processed 2570000 lines, 0 errors\n",
            "Processed 2580000 lines, 0 errors\n",
            "Processed 2590000 lines, 0 errors\n",
            "Processed 2600000 lines, 0 errors\n",
            "Processed 2610000 lines, 0 errors\n",
            "Processed 2620000 lines, 0 errors\n",
            "Processed 2630000 lines, 0 errors\n",
            "Processed 2640000 lines, 0 errors\n",
            "Processed 2650000 lines, 0 errors\n",
            "Processed 2660000 lines, 0 errors\n",
            "Processed 2670000 lines, 0 errors\n",
            "Processed 2680000 lines, 0 errors\n",
            "Processed 2690000 lines, 0 errors\n",
            "Processed 2700000 lines, 0 errors\n",
            "Processed 2710000 lines, 0 errors\n",
            "Processed 2720000 lines, 0 errors\n",
            "Processed 2730000 lines, 0 errors\n",
            "Processed 2740000 lines, 0 errors\n",
            "Processed 2750000 lines, 0 errors\n",
            "Processed 2760000 lines, 0 errors\n",
            "Processed 2770000 lines, 0 errors\n",
            "Processed 2780000 lines, 0 errors\n",
            "Processed 2790000 lines, 0 errors\n",
            "Processed 2800000 lines, 0 errors\n",
            "Processed 2810000 lines, 0 errors\n",
            "Processed 2820000 lines, 0 errors\n",
            "Processed 2830000 lines, 0 errors\n",
            "Processed 2840000 lines, 0 errors\n",
            "Processed 2850000 lines, 0 errors\n",
            "Processed 2860000 lines, 0 errors\n",
            "Processed 2870000 lines, 0 errors\n",
            "Processed 2880000 lines, 0 errors\n",
            "Processed 2890000 lines, 0 errors\n",
            "Processed 2900000 lines, 0 errors\n",
            "Processed 2910000 lines, 0 errors\n",
            "Processed 2920000 lines, 0 errors\n",
            "Processed 2930000 lines, 0 errors\n",
            "Processed 2940000 lines, 0 errors\n",
            "Processed 2950000 lines, 0 errors\n",
            "Processed 2960000 lines, 0 errors\n",
            "Processed 2970000 lines, 0 errors\n",
            "Processed 2980000 lines, 0 errors\n",
            "Processed 2990000 lines, 0 errors\n",
            "Processed 3000000 lines, 0 errors\n",
            "Processed 3010000 lines, 0 errors\n",
            "Processed 3020000 lines, 0 errors\n",
            "Processed 3030000 lines, 0 errors\n",
            "Processed 3040000 lines, 0 errors\n",
            "Processed 3050000 lines, 0 errors\n",
            "Processed 3060000 lines, 0 errors\n",
            "Processed 3070000 lines, 0 errors\n",
            "Processed 3080000 lines, 0 errors\n",
            "Processed 3090000 lines, 0 errors\n",
            "Processed 3100000 lines, 0 errors\n",
            "Processed 3110000 lines, 0 errors\n",
            "Processed 3120000 lines, 0 errors\n",
            "Processed 3130000 lines, 0 errors\n",
            "Processed 3140000 lines, 0 errors\n",
            "Processed 3150000 lines, 0 errors\n",
            "Processed 3160000 lines, 0 errors\n",
            "Processed 3170000 lines, 0 errors\n",
            "Processed 3180000 lines, 0 errors\n",
            "Processed 3190000 lines, 0 errors\n",
            "Processed 3200000 lines, 0 errors\n",
            "Processed 3210000 lines, 0 errors\n",
            "Processed 3220000 lines, 0 errors\n",
            "Processed 3230000 lines, 0 errors\n",
            "Processed 3240000 lines, 0 errors\n",
            "Processed 3250000 lines, 0 errors\n",
            "Processed 3260000 lines, 0 errors\n",
            "Processed 3270000 lines, 0 errors\n",
            "Processed 3280000 lines, 0 errors\n",
            "Processed 3290000 lines, 0 errors\n",
            "Processed 3300000 lines, 0 errors\n",
            "Processed 3310000 lines, 0 errors\n",
            "Processed 3320000 lines, 0 errors\n",
            "Processed 3330000 lines, 0 errors\n",
            "Processed 3340000 lines, 0 errors\n",
            "Processed 3350000 lines, 0 errors\n",
            "Processed 3360000 lines, 0 errors\n",
            "Processed 3370000 lines, 0 errors\n",
            "Processed 3380000 lines, 0 errors\n",
            "Processed 3390000 lines, 0 errors\n",
            "Processed 3400000 lines, 0 errors\n",
            "Processed 3410000 lines, 0 errors\n",
            "Processed 3420000 lines, 0 errors\n",
            "Processed 3430000 lines, 0 errors\n",
            "Processed 3440000 lines, 0 errors\n",
            "Processed 3450000 lines, 0 errors\n",
            "Processed 3460000 lines, 0 errors\n",
            "Processed 3470000 lines, 0 errors\n",
            "Processed 3480000 lines, 0 errors\n",
            "Processed 3490000 lines, 0 errors\n",
            "Processed 3500000 lines, 0 errors\n",
            "Processed 3510000 lines, 0 errors\n",
            "Processed 3520000 lines, 0 errors\n",
            "Processed 3530000 lines, 0 errors\n",
            "Processed 3540000 lines, 0 errors\n",
            "Processed 3550000 lines, 0 errors\n",
            "Processed 3560000 lines, 0 errors\n",
            "Processed 3570000 lines, 0 errors\n",
            "Processed 3580000 lines, 0 errors\n",
            "Processed 3590000 lines, 0 errors\n",
            "Processed 3600000 lines, 0 errors\n",
            "Processed 3610000 lines, 0 errors\n",
            "Processed 3620000 lines, 0 errors\n",
            "Processed 3630000 lines, 0 errors\n",
            "Processed 3640000 lines, 0 errors\n",
            "Processed 3650000 lines, 0 errors\n",
            "Processed 3660000 lines, 0 errors\n",
            "Processed 3670000 lines, 0 errors\n",
            "Processed 3680000 lines, 0 errors\n",
            "Processed 3690000 lines, 0 errors\n",
            "Processed 3700000 lines, 0 errors\n",
            "Processed 3710000 lines, 0 errors\n",
            "Processed 3720000 lines, 0 errors\n",
            "Processed 3730000 lines, 0 errors\n",
            "Processed 3740000 lines, 0 errors\n",
            "Processed 3750000 lines, 0 errors\n",
            "Processed 3760000 lines, 0 errors\n",
            "Processed 3770000 lines, 0 errors\n",
            "Processed 3780000 lines, 0 errors\n",
            "Processed 3790000 lines, 0 errors\n",
            "Processed 3800000 lines, 0 errors\n",
            "Processed 3810000 lines, 0 errors\n",
            "Processed 3820000 lines, 0 errors\n",
            "Processed 3830000 lines, 0 errors\n",
            "Processed 3840000 lines, 0 errors\n",
            "Processed 3850000 lines, 0 errors\n",
            "Processed 3860000 lines, 0 errors\n",
            "Processed 3870000 lines, 0 errors\n",
            "Processed 3880000 lines, 0 errors\n",
            "Processed 3890000 lines, 0 errors\n",
            "Processed 3900000 lines, 0 errors\n",
            "Processed 3910000 lines, 0 errors\n",
            "Processed 3920000 lines, 0 errors\n",
            "Processed 3930000 lines, 0 errors\n",
            "Processed 3940000 lines, 0 errors\n",
            "Processed 3950000 lines, 0 errors\n",
            "Processed 3960000 lines, 0 errors\n",
            "Processed 3970000 lines, 0 errors\n",
            "Processed 3980000 lines, 0 errors\n",
            "Processed 3990000 lines, 0 errors\n",
            "Processed 4000000 lines, 0 errors\n",
            "\n",
            "Fixed CSV saved to: fixed_data.csv\n",
            "Total rows: 1885670, Errors: 0\n",
            "================================================================================\n",
            "AI TEXT DETECTION PIPELINE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "DATA PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "Initial shape: (1885670, 2)\n",
            "Columns: ['text', 'generated']\n",
            "Removed 645945 duplicate rows\n",
            "Removed 6387 rows with null values\n",
            "Removed 975764 rows with invalid labels\n",
            "Removed 5441 rows with text < 50 characters\n",
            "Removed 0 rows with text > 10000 characters\n",
            "\n",
            "Final dataset shape: (252133, 2)\n",
            "\n",
            "Class distribution:\n",
            "generated\n",
            "0    171491\n",
            "1     80642\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution (%):\n",
            "generated\n",
            "0    68.016087\n",
            "1    31.983913\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Extracting linguistic features...\n",
            "\n",
            "================================================================================\n",
            "TRAINING TRADITIONAL ML MODELS\n",
            "================================================================================\n",
            "\n",
            "Creating TF-IDF features...\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.9483\n",
            "\n",
            "Training XGBoost...\n",
            "XGBoost Accuracy: 0.9721\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.9613\n",
            "\n",
            "Creating Ensemble Model...\n"
          ]
        }
      ]
    }
  ]
}